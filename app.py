import os
import streamlit as st
import pandas as pd
import xml.etree.ElementTree as ET
import joblib
from imblearn.over_sampling import ADASYN
import emoji
from nltk.tokenize import word_tokenize, RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
import re
import unicodedata
import contractions
from sklearn.base import BaseEstimator, TransformerMixin
import nltk
from imblearn.over_sampling import ADASYN
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier


# –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK
nltk.download('stopwords')
nltk.download('punkt')

# –ö–ª–∞—Å—Å –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ADASYN
# class ADASYNTransformer(BaseEstimator, TransformerMixin):
#     def __init__(self):
#         self.adasyn = ADASYN()

#     def fit(self, X, y=None):
#         # –ù–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º –≤ fit, —Ç–∞–∫ –∫–∞–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ transform
#         return self

#     def transform(self, X):
#         if isinstance(X, pd.Series):
#             print(f"Transforming {len(X)} items...")
#             return X.apply(self.preprocess_text)
#         else:
#             raise ValueError("Input is not a pandas Series")

# –ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞


class PreprocessText(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.stemmer = SnowballStemmer('russian')
        self.stop_words = set()
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤
            self.stop_words = set(stopwords.words('russian'))
            print(f"Stopwords loaded: {len(self.stop_words)} stop words.")
        except Exception as e:
            print(f"Error loading stopwords: {e}")

    def emojis_words(self, text):
        clean_text = emoji.demojize(text, delimiters=(" ", " "))
        clean_text = clean_text.replace(":", "").replace("_", " ")
        return clean_text

    def preprocess_text(self, text):
        if not isinstance(text, str):
            return ''
        text = re.sub('<[^<]+?>', '', text)
        text = re.sub(r'http\S+', '', text)
        text = self.emojis_words(text)
        text = text.lower()
        text = re.sub('\s+', ' ', text)
        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        text = contractions.fix(text)
        text = re.sub('[^a-zA-Z0-9\s]', '', text)

        tokens = nltk.word_tokenize(text)
        if hasattr(self, 'stop_words') and self.stop_words:  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è stop_words
            tokens = [token for token in tokens if token not in self.stop_words]
        else:
            print("Warning: Stop words are not initialized correctly.")
        
        stem_tokens = [self.stemmer.stem(word) for word in tokens]
        return ' '.join(stem_tokens)

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        if isinstance(X, pd.Series):
            print(f"Transforming {len(X)} items...")
            return X.apply(self.preprocess_text)
        else:
            raise ValueError("Input is not a pandas Series")

 #–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
def clean_text(text):
    if not isinstance(text, str):
        text = str(text)
    text = re.sub(r'(_x000D_|\r|\n)', ' ', text)
    return text.strip()


# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã Streamlit
st.set_page_config(layout="wide", page_title="Force Line", page_icon="‚ö°")

# –ó–∞–≥—Ä—É–∑–∫–∞ CSS –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ —Å—Ç–∏–ª—è
def load_css(file_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç CSS –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.""" 
    with open(file_path) as f:
        st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)

css_file = 'style.css'
load_css(css_file)

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
with st.sidebar:
    st.image("logo.png", use_column_width=True)  # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ª–æ–≥–æ—Ç–∏–ø–∞
    st.title("‚ö° Force Line")
    choice = st.radio("–ù–∞–≤–∏–≥–∞—Ü–∏—è", ["–ó–∞–≥—Ä—É–∑–∫–∞",  "–ó–∞—è–≤–∫–∞", "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è", "–≠–∫—Å–ø–æ—Ä—Ç"])
    st.info("ü§ñ –ü—Ä–æ–≥—Ä–∞–º–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞—è–≤–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ–±—â–µ–Ω–∏–π AP, –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –ø–æ—á—Ç—ã.")

# –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
st.markdown("<h1 style='color: #d51d29;'>–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ Force Line! ‚ö°</h1>", unsafe_allow_html=True)
st.markdown("üëã –ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∑–∞—è–≤–æ–∫, –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –ø–æ —Ç–∏–ø–∞–º –∏ –≤—ã–¥–µ–ª—è—Ç—å –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
if os.path.exists('./dataset.csv'):
    df = pd.read_csv('dataset.csv', index_col=None)
else:
    df = pd.DataFrame()  # –ü—É—Å—Ç–æ–π DataFrame –¥–ª—è —Å–ª—É—á–∞–µ–≤, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –µ—â–µ –Ω–µ—Ç

# –ë–ª–æ–∫ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
if choice == "–ó–∞–≥—Ä—É–∑–∫–∞":
    st.title("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    files = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã —Å –∑–∞—è–≤–∫–∞–º–∏ (–º–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤)", type=["csv", "xlsx", "xls"], accept_multiple_files=True)

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
    def read_file(uploaded_file):
        try:
            if uploaded_file.name.endswith('.csv'):
                return pd.read_csv(uploaded_file, sep=',', index_col=None)
            else:
                return pd.read_excel(uploaded_file, index_col=None)
        except pd.errors.EmptyDataError:
            st.error("üö® –§–∞–π–ª –ø—É—Å—Ç–æ–π. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥—Ä—É–≥–æ–π —Ñ–∞–π–ª.")
        except Exception as e:
            st.error(f"üö® –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {uploaded_file.name}: {e}")
        return None

    # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤
    dataframes = []

    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
    if files:
        for file in files:
            df = read_file(file)
            if df is not None:
                dataframes.append(df)
                st.success(f"–§–∞–π–ª {file.name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω! üéâ")

        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤
        if dataframes:
            combined_df = pd.concat(dataframes, ignore_index=True)
            combined_file_name = 'combined_dataset.csv'
            combined_df.to_csv(combined_file_name, index=None)
            st.success("–í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: combined_dataset.csv")

            # –ö–Ω–æ–ø–∫–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
            st.download_button(
                label="üì• –°–∫–∞—á–∞—Ç—å –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π —Ñ–∞–π–ª",
                data=combined_df.to_csv(index=False).encode('utf-8'),
                file_name='combined_dataset.csv',
                mime='text/csv'
            )

            st.session_state.combined_df = combined_df
            

# –°—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞—è–≤–∫–∏
if choice == "–ó–∞—è–≤–∫–∞":
    st.title("üìë –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∑–∞—è–≤–∫–∏")
    st.subheader("–í–≤–µ–¥–∏—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞—è–≤–∫–µ")

    # –í–≤–æ–¥ —Ç–µ–º—ã –∑–∞—è–≤–∫–∏ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ)
    —Ç–µ–º–∞ = st.text_input("–¢–µ–º–∞ –∑–∞—è–≤–∫–∏", placeholder="–í–≤–µ–¥–∏—Ç–µ —Ç–µ–º—É –∑–∞—è–≤–∫–∏", max_chars=100)

    # –í–≤–æ–¥ –æ–ø–∏—Å–∞–Ω–∏—è –∑–∞—è–≤–∫–∏ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ)
    –æ–ø–∏—Å–∞–Ω–∏–µ = st.text_area("–û–ø–∏—Å–∞–Ω–∏–µ –∑–∞—è–≤–∫–∏", placeholder="–û–ø–∏—à–∏—Ç–µ –ø—Ä–æ–±–ª–µ–º—É –ø–æ–¥—Ä–æ–±–Ω–æ", height=200)

    # –í—ã–ø–∞–¥–∞—é—â–∏–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ç–∏–ø–∞ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ)
    —Ç–∏–ø_–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è = st.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è", 
        ["–ù–æ—É—Ç–±—É–∫", "–°–µ—Ä–≤–µ—Ä", "–†–∞–±–æ—á–∞—è —Å—Ç–∞–Ω—Ü–∏—è", "–ü—Ä–∏–Ω—Ç–µ—Ä", "–î—Ä—É–≥–æ–π"]
    )

    # –í—ã–ø–∞–¥–∞—é—â–∏–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ç–æ—á–∫–∏ –æ—Ç–∫–∞–∑–∞ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ)
    —Ç–æ—á–∫–∞_–æ—Ç–∫–∞–∑–∞ = st.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ —Ç–æ—á–∫—É –æ—Ç–∫–∞–∑–∞", 
        ["–ë–ª–æ–∫ –ø–∏—Ç–∞–Ω–∏—è", "–ú–∞—Ç–µ—Ä–∏–Ω—Å–∫–∞—è –ø–ª–∞—Ç–∞", "–ú–∞—Ç—Ä–∏—Ü–∞", "–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä", "–î—Ä—É–≥–æ–π"]
    )

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–µ—Ä–∏–π–Ω–æ–≥–æ –Ω–æ–º–µ—Ä–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
    def extract_serial_number(text):
        match = re.search(r'\b[A-Z0-9]{8,}\b', text)
        return match.group(0) if match else "–ù–µ —É–∫–∞–∑–∞–Ω"

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ—Ä–∏–π–Ω–æ–≥–æ –Ω–æ–º–µ—Ä–∞ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ)
    —Å–µ—Ä–∏–π–Ω—ã–π_–Ω–æ–º–µ—Ä = extract_serial_number(–æ–ø–∏—Å–∞–Ω–∏–µ)
    if —Å–µ—Ä–∏–π–Ω—ã–π_–Ω–æ–º–µ—Ä == "–ù–µ —É–∫–∞–∑–∞–Ω":
        —Å–µ—Ä–∏–π–Ω—ã–π_–Ω–æ–º–µ—Ä = st.text_input("–°–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä", placeholder="–í–≤–µ–¥–∏—Ç–µ —Å–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä (–µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω)")

    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞—è–≤–∫–∏ –ø–æ —Ç–∏–ø—É (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ)
    if "–æ—à–∏–±–∫–∞" in –æ–ø–∏—Å–∞–Ω–∏–µ.lower():
        —Ç–∏–ø_–∑–∞—è–≤–∫–∏ = "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è"
    else:
        —Ç–∏–ø_–∑–∞—è–≤–∫–∏ = "–û–±—â–∞—è"

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    if st.button("–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∑–∞—è–≤–∫—É"):
        st.write(f"**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏**: ")
        st.write(f"–¢–µ–º–∞ –∑–∞—è–≤–∫–∏: {—Ç–µ–º–∞}")
        st.write(f"–û–ø–∏—Å–∞–Ω–∏–µ –∑–∞—è–≤–∫–∏: {–æ–ø–∏—Å–∞–Ω–∏–µ}")
        st.write(f"–¢–∏–ø –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è: {—Ç–∏–ø_–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è}")
        st.write(f"–¢–æ—á–∫–∞ –æ—Ç–∫–∞–∑–∞: {—Ç–æ—á–∫–∞_–æ—Ç–∫–∞–∑–∞}")
        st.write(f"–°–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä: {—Å–µ—Ä–∏–π–Ω—ã–π_–Ω–æ–º–µ—Ä}")
        st.write(f"–¢–∏–ø –∑–∞—è–≤–∫–∏: {—Ç–∏–ø_–∑–∞—è–≤–∫–∏}")

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ DataFrame
        new_row = {
            "–¢–µ–º–∞": —Ç–µ–º–∞,
            "–û–ø–∏—Å–∞–Ω–∏–µ": –æ–ø–∏—Å–∞–Ω–∏–µ,
            "–¢–∏–ø –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è": —Ç–∏–ø_–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è,
            "–¢–æ—á–∫–∞ –æ—Ç–∫–∞–∑–∞": —Ç–æ—á–∫–∞_–æ—Ç–∫–∞–∑–∞,
            "–°–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä": —Å–µ—Ä–∏–π–Ω—ã–π_–Ω–æ–º–µ—Ä
        }
        df = df.append(new_row, ignore_index=True)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ DataFrame –≤ session_state
        st.session_state.combined_df = df

        st.success("–ó–∞—è–≤–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

        # –í—ã–≤–æ–¥ —Ç–∞–±–ª–∏—Ü—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        st.write("–¢–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∑–∞—è–≤–∫–∏: ")
        st.dataframe(df)

def get_predict(text):
    return model.predict(pd.Series([text]))[0] 

# –°—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞—è–≤–æ–∫
if choice == "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è":
    st.title("üìå –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞—è–≤–æ–∫")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    if 'model' not in st.session_state:
        model_path = "D:\\ForceLine3000\\catboost_pipeline3.pkl"
        if os.path.exists(model_path):
            st.session_state.model = joblib.load(model_path)
            st.success("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        else:
            st.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–∏.")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
    if 'model' in st.session_state:
        model = st.session_state.model

        if 'combined_df' in st.session_state and not st.session_state.combined_df.empty:
            df = st.session_state.combined_df
            df['–¢–µ–∫—Å—Ç'] = df['–¢–µ–º–∞']+' '+df['–û–ø–∏—Å–∞–Ω–∏–µ']
            df['–ö–ª–∞—Å—Å'] = df['–¢–µ–∫—Å—Ç'].apply(get_predict)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º '–ö–ª–∞—Å—Å' –≤ —Å—Ç—Ä–æ–∫–æ–≤—ã–π —Ç–∏–ø
            # df['–ö–ª–∞—Å—Å'] = df['–ö–ª–∞—Å—Å'].astype(str)
      
            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
            st.write("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞—è–≤–æ–∫: ")
            
            st.dataframe(df[['–¢–µ–∫—Å—Ç', '–û–ø–∏—Å–∞–Ω–∏–µ', '–ö–ª–∞—Å—Å']])
        else:
            st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.")
    else:
        st.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞! –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å.")

# –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –≤ CSV –∏ XML
if choice == "–≠–∫—Å–ø–æ—Ä—Ç":
    st.title("üíæ –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")
    
    # –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV
    if 'combined_df' in st.session_state:
        df = st.session_state.combined_df
        csv_data = df.to_csv(index=False)
        st.download_button(label="üìÇ –°–∫–∞—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ CSV", data=csv_data, file_name="export_data.csv", mime="text/csv")
    
        # –≠–∫—Å–ø–æ—Ä—Ç –≤ XML
        def to_xml(df):
            """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ XML –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ 1–°.""" 
            root = ET.Element("Data")
            for _, row in df.iterrows():
                entry = ET.SubElement(root, "Entry")
                for col in df.columns:
                    ET.SubElement(entry, col).text = str(row[col])
            return ET.tostring(root, encoding="unicode")

        xml_data = to_xml(df)
        st.download_button(label="üìÇ –°–∫–∞—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ XML", data=xml_data, file_name="export_data.xml", mime="application/xml")

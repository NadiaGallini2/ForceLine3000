import os
import streamlit as st
import pandas as pd
import xml.etree.ElementTree as ET
import joblib
from imblearn.over_sampling import ADASYN
import emoji
from nltk.tokenize import word_tokenize, RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
import re
import unicodedata
import contractions
from sklearn.base import BaseEstimator, TransformerMixin
import nltk
from imblearn.over_sampling import ADASYN
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier




# –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK
nltk.download('stopwords')
nltk.download('punkt')

# –ö–ª–∞—Å—Å –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ADASYN
class ADASYNTransformer(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.adasyn = ADASYN()

    def fit(self, X, y=None):
        # –ù–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º –≤ fit, —Ç–∞–∫ –∫–∞–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ transform
        return self

    def transform(self, X, y=None):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ y –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
        if y is None:
            raise ValueError("y –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ–¥–∞–Ω –¥–ª—è ADASYN —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º X –∏ y
        X_res, y_res = self.adasyn.fit_resample(X, y)
        return X_res, y_res

# –ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
class PreprocessText(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.stemmer = SnowballStemmer('russian')
        self.stop_words = set()
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤
            self.stop_words = set(stopwords.words('russian'))
            print(f"Stopwords loaded: {len(self.stop_words)} stop words.")
        except Exception as e:
            print(f"Error loading stopwords: {e}")

    def emojis_words(self, text):
        clean_text = emoji.demojize(text, delimiters=(" ", " "))
        clean_text = clean_text.replace(":", "").replace("_", " ")
        return clean_text

    def preprocess_text(self, text):
        if not isinstance(text, str):
            return ''
        text = re.sub('<[^<]+?>', '', text)
        text = re.sub(r'http\S+', '', text)
        text = self.emojis_words(text)
        text = text.lower()
        text = re.sub('\s+', ' ', text)
        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        text = contractions.fix(text)
        text = re.sub('[^a-zA-Z0-9\s]', '', text)

        tokens = nltk.word_tokenize(text)
        if hasattr(self, 'stop_words') and self.stop_words:  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è stop_words
            tokens = [token for token in tokens if token not in self.stop_words]
        else:
            print("Warning: Stop words are not initialized correctly.")
        
        stem_tokens = [self.stemmer.stem(word) for word in tokens]
        return ' '.join(stem_tokens)

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        if isinstance(X, pd.Series):
            print(f"Transforming {len(X)} items...")
            return X.apply(self.preprocess_text)
        else:
            raise ValueError("Input is not a pandas Series")

 #–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
def clean_text(text):
    if not isinstance(text, str):
        text = str(text)
    text = re.sub(r'(_x000D_|\r|\n)', ' ', text)
    return text.strip()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã Streamlit
st.set_page_config(layout="wide", page_title="Force Line", page_icon="‚ö°")

# –ó–∞–≥—Ä—É–∑–∫–∞ CSS –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ —Å—Ç–∏–ª—è
def load_css(file_path):
    with open(file_path) as f:
        st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)

css_file = 'style.css'
load_css(css_file)

# –ù–∞–≤–∏–≥–∞—Ü–∏—è –∏ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ
with st.sidebar:
    st.image("logo.png", use_column_width=True)
    st.title("‚ö° Force Line")
    choice = st.radio("–ù–∞–≤–∏–≥–∞—Ü–∏—è", ["–ó–∞–≥—Ä—É–∑–∫–∞",  "–ó–∞—è–≤–∫–∞", "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è", "–≠–∫—Å–ø–æ—Ä—Ç"])

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
if choice == "–ó–∞–≥—Ä—É–∑–∫–∞":
    st.title("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    files = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã —Å –∑–∞—è–≤–∫–∞–º–∏", type=["csv", "xlsx", "xls"], accept_multiple_files=True)

    def read_file(uploaded_file):
        try:
            if uploaded_file.name.endswith('.csv'):
                return pd.read_csv(uploaded_file)
            else:
                return pd.read_excel(uploaded_file)
        except Exception as e:
            st.error(f"üö® –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        return None

    dataframes = [read_file(file) for file in files if file]
    if dataframes:
        combined_df = pd.concat(dataframes, ignore_index=True)
        st.session_state.combined_df = combined_df
        st.download_button("üì• –°–∫–∞—á–∞—Ç—å –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π —Ñ–∞–π–ª", data=combined_df.to_csv(index=False).encode('utf-8'), file_name='combined_dataset.csv', mime='text/csv')

# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞—è–≤–∫–∏
if choice == "–ó–∞—è–≤–∫–∞":
    st.title("üìë –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∑–∞—è–≤–∫–∏")
    st.subheader("–í–≤–µ–¥–∏—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞—è–≤–∫–µ")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–≥–æ DataFrame, –µ—Å–ª–∏ –æ–Ω –µ—â–µ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    if "combined_df" not in st.session_state:
        st.session_state.combined_df = pd.DataFrame(columns=["–¢–µ–º–∞", "–û–ø–∏—Å–∞–Ω–∏–µ", "–¢–∏–ø –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è", "–¢–æ—á–∫–∞ –æ—Ç–∫–∞–∑–∞", "–°–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä"])

    # –í–≤–æ–¥ —Ç–µ–º—ã –∑–∞—è–≤–∫–∏ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ)
    —Ç–µ–º–∞ = st.text_input("–¢–µ–º–∞ –∑–∞—è–≤–∫–∏", placeholder="–í–≤–µ–¥–∏—Ç–µ —Ç–µ–º—É –∑–∞—è–≤–∫–∏", max_chars=100)

    # –í–≤–æ–¥ –æ–ø–∏—Å–∞–Ω–∏—è –∑–∞—è–≤–∫–∏ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ)
    –æ–ø–∏—Å–∞–Ω–∏–µ = st.text_area("–û–ø–∏—Å–∞–Ω–∏–µ –∑–∞—è–≤–∫–∏", placeholder="–û–ø–∏—à–∏—Ç–µ –ø—Ä–æ–±–ª–µ–º—É –ø–æ–¥—Ä–æ–±–Ω–æ", height=200)

    # –í—ã–ø–∞–¥–∞—é—â–∏–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ç–∏–ø–∞ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ)
    —Ç–∏–ø_–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è = st.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è", 
        ["–ù–æ—É—Ç–±—É–∫", "–°–µ—Ä–≤–µ—Ä", "–†–∞–±–æ—á–∞—è —Å—Ç–∞–Ω—Ü–∏—è", "–ü—Ä–∏–Ω—Ç–µ—Ä", "–î—Ä—É–≥–æ–π"]
    )

    # –í—ã–ø–∞–¥–∞—é—â–∏–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ç–æ—á–∫–∏ –æ—Ç–∫–∞–∑–∞ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ)
    —Ç–æ—á–∫–∞_–æ—Ç–∫–∞–∑–∞ = st.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ —Ç–æ—á–∫—É –æ—Ç–∫–∞–∑–∞", 
        ["–ë–ª–æ–∫ –ø–∏—Ç–∞–Ω–∏—è", "–ú–∞—Ç–µ—Ä–∏–Ω—Å–∫–∞—è –ø–ª–∞—Ç–∞", "–ú–∞—Ç—Ä–∏—Ü–∞", "–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä", "–î—Ä—É–≥–æ–π"]
    )

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–µ—Ä–∏–π–Ω–æ–≥–æ –Ω–æ–º–µ—Ä–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
    def extract_serial_number(text):
        match = re.search(r'\b[A-Z0-9]{8,}\b', text)
        return match.group(0) if match else "–ù–µ —É–∫–∞–∑–∞–Ω"

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ—Ä–∏–π–Ω–æ–≥–æ –Ω–æ–º–µ—Ä–∞ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ)
    —Å–µ—Ä–∏–π–Ω—ã–π_–Ω–æ–º–µ—Ä = extract_serial_number(–æ–ø–∏—Å–∞–Ω–∏–µ)
    if —Å–µ—Ä–∏–π–Ω—ã–π_–Ω–æ–º–µ—Ä == "–ù–µ —É–∫–∞–∑–∞–Ω":
        —Å–µ—Ä–∏–π–Ω—ã–π_–Ω–æ–º–µ—Ä = st.text_input("–°–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä", placeholder="–í–≤–µ–¥–∏—Ç–µ —Å–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä (–µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω)")

    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞—è–≤–∫–∏ –ø–æ —Ç–∏–ø—É (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ)
    if "–æ—à–∏–±–∫–∞" in –æ–ø–∏—Å–∞–Ω–∏–µ.lower():
        —Ç–∏–ø_–∑–∞—è–≤–∫–∏ = "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è"
    else:
        —Ç–∏–ø_–∑–∞—è–≤–∫–∏ = "–û–±—â–∞—è"

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    if st.button("–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∑–∞—è–≤–∫—É"):
        st.write(f"**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏**:")
        st.write(f"–¢–µ–º–∞ –∑–∞—è–≤–∫–∏: {—Ç–µ–º–∞}")
        st.write(f"–û–ø–∏—Å–∞–Ω–∏–µ –∑–∞—è–≤–∫–∏: {–æ–ø–∏—Å–∞–Ω–∏–µ}")
        st.write(f"–¢–∏–ø –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è: {—Ç–∏–ø_–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è}")
        st.write(f"–¢–æ—á–∫–∞ –æ—Ç–∫–∞–∑–∞: {—Ç–æ—á–∫–∞_–æ—Ç–∫–∞–∑–∞}")
        st.write(f"–°–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä: {—Å–µ—Ä–∏–π–Ω—ã–π_–Ω–æ–º–µ—Ä}")
        st.write(f"–¢–∏–ø –∑–∞—è–≤–∫–∏: {—Ç–∏–ø_–∑–∞—è–≤–∫–∏}")

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ DataFrame
        new_row = {
            "–¢–µ–º–∞": —Ç–µ–º–∞,
            "–û–ø–∏—Å–∞–Ω–∏–µ": –æ–ø–∏—Å–∞–Ω–∏–µ,
            "–¢–∏–ø –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è": —Ç–∏–ø_–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è,
            "–¢–æ—á–∫–∞ –æ—Ç–∫–∞–∑–∞": —Ç–æ—á–∫–∞_–æ—Ç–∫–∞–∑–∞,
            "–°–µ—Ä–∏–π–Ω—ã–π –Ω–æ–º–µ—Ä": —Å–µ—Ä–∏–π–Ω—ã–π_–Ω–æ–º–µ—Ä
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π DataFrame
        st.session_state.combined_df = st.session_state.combined_df.append(new_row, ignore_index=True)

        st.success("–ó–∞—è–≤–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

        # –í—ã–≤–æ–¥ —Ç–∞–±–ª–∏—Ü—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        st.write("–¢–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∑–∞—è–≤–∫–∏:")
        st.dataframe(st.session_state.combined_df)

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞—è–≤–æ–∫
if choice == "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è":
    st.title("üîç –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞—è–≤–æ–∫")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –æ–Ω–∞ –µ—â—ë –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
    if 'model' not in st.session_state:
        try:
            st.session_state.model = joblib.load("D:\\ForceLine3000\\catboost_pipeline.pkl")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö
    if 'combined_df' in st.session_state:
        df = st.session_state.combined_df
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ '–û–ø–∏—Å–∞–Ω–∏–µ'
        if df['–û–ø–∏—Å–∞–Ω–∏–µ'].isnull().sum() > 0:
            st.error("‚ö†Ô∏è –í —Å—Ç–æ–ª–±—Ü–µ '–û–ø–∏—Å–∞–Ω–∏–µ' –µ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è!")
            st.stop()

        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–¥–∞—á–µ–π –≤ –º–æ–¥–µ–ª—å
        df['–û–ø–∏—Å–∞–Ω–∏–µ'] = df['–û–ø–∏—Å–∞–Ω–∏–µ'].apply(lambda x: str(x) if isinstance(x, str) else '')

        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –∑–∞—è–≤–æ–∫
        df['predicted_category'] = st.session_state.model.predict(df['–û–ø–∏—Å–∞–Ω–∏–µ'])
        st.write("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞—è–≤–æ–∫:")
        st.dataframe(df[['–¢–µ–º–∞', '–û–ø–∏—Å–∞–Ω–∏–µ', 'predicted_category']])

    else:
        st.warning("–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.")

# –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö
if choice == "–≠–∫—Å–ø–æ—Ä—Ç":
    if 'combined_df' in st.session_state:
        df = st.session_state.combined_df
        csv_data = df.to_csv(index=False)
        st.download_button("üìÇ –°–∫–∞—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ CSV", data=csv_data, file_name="export_data.csv", mime="text/csv")

        def to_xml(df):
            root = ET.Element("Data")
            for _, row in df.iterrows():
                entry = ET.SubElement(root, "Entry")
                for col_name, col_value in row.items():
                    col_element = ET.SubElement(entry, col_name)
                    col_element.text = str(col_value)
            return ET.tostring(root, encoding="utf-8")

        xml_data = to_xml(df)
        st.download_button("üìÇ –°–∫–∞—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ XML", data=xml_data, file_name="export_data.xml", mime="application/xml")
    else:
        st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ.")
